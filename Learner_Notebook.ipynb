{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef1ab2a-0ee3-41cd-ae1e-75f53bffb278",
   "metadata": {},
   "source": [
    "# Building your first neural network in Pyton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173bf02e-0f6f-4559-aa85-1b78863ba352",
   "metadata": {},
   "source": [
    "## Forward Propagation in a Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8dd13-2d4e-4730-a23c-f33a84210c70",
   "metadata": {},
   "source": [
    "**Forward propagation** is the process a neural network uses to generate predictions from input data. It‚Äôs the first step in learning, taking an input, passing it through hidden layers using weights, biases, and activation functions, and producing an output.\n",
    "\n",
    "In simple terms, it answers this question:\n",
    "**What does the network predict given this input and these weights?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae6474-355a-4650-ac15-626da0b4b000",
   "metadata": {},
   "source": [
    "Think of **forward propagation** like making a smoothie:\n",
    "\n",
    "- The **inputs** (like bananas, strawberries, and milk) are your features.\n",
    "\n",
    "- The blender is the **neural network**: it mixes the ingredients using weights (how much of each ingredient to use) and biases (extra flavor).\n",
    "\n",
    "- The **activation functions** are like taste filters ‚Äî they adjust the flavor before you pour the smoothie.\n",
    "\n",
    "- The final smoothie is your **output**, a prediction based on the ingredients you put in and how you blended them.\n",
    "\n",
    "In **forward propagation**, you're just blending inputs through the network to get a result, **no learning yet**, just tasting what the current recipe gives you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f202e-bbde-4c9b-ba2e-3ba326f82b29",
   "metadata": {},
   "source": [
    "### A Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d445e46-02dc-4ce2-9542-098b48cd1df5",
   "metadata": {},
   "source": [
    "In order to understand **forward propagation** Let‚Äôs break down the parts of a simple neural network:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/alejo1630/assessment_NN/refs/heads/main/Images/NN.jpg\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e6d796-eeaf-48c2-95f9-149bded2be7f",
   "metadata": {},
   "source": [
    "**üü† INPUT LAYER**\n",
    "\n",
    "This is where the data enters the network. Each **node** represents a feature from your dataset. The input layer **doesn't process**, it just passes the values forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b840643-cd63-428f-a005-e7cdb36729f5",
   "metadata": {},
   "source": [
    "**üîµ HIDDEN LAYER**\n",
    "\n",
    "This is where the *\"thinking\"* happens. Each neuron in the hidden layer:\n",
    "\n",
    "- Multiplies the inputs by their weight $(w_i)$ and combines them in a weighted sum $(Z)$, and adds the bias term,\n",
    "\n",
    "- Applies an activation function $(f)$\n",
    "\n",
    "The **goal**: to learn patterns from the input. \n",
    "\n",
    "Even though this example shows just one neuron in the hidden layer (this neunal network is called Perceptron), other network can have many hidden layers stacked together and each hiiden layer can have one or more neurons or nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35be302-b868-44bc-bd94-5f127baf5e73",
   "metadata": {},
   "source": [
    "**üü° BIAS**\n",
    "\n",
    "Biases are like an adjustable offset. They allow the network to shift the output up or down, making learning more flexible.\n",
    "\n",
    "Think of them like seasoning in a recipe: Even with the same ingredients (inputs), a little salt (bias) can make a big difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c037a24-5e0f-439b-84ec-e03f141df3e6",
   "metadata": {},
   "source": [
    "**üü¢ OUTPUT LAYER**\n",
    "\n",
    "This is where the final prediction happens. It combines everything the hidden layers have learned and produces:\n",
    "\n",
    "- A class (binary classification)\n",
    "\n",
    "- A number (regression)\n",
    "\n",
    "- A probability distribution (multiclass classificacition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f521853c-8b5c-4dac-b314-553fe530757a",
   "metadata": {},
   "source": [
    "### Math Behind Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7c000-5085-4af4-bb73-38e0f0aa8cb4",
   "metadata": {},
   "source": [
    "Behind every neural network prediction lies a simple but powerful sequence of mathematical operations. Understanding this process ‚Äî starting with how we compute will give you the tools to not just use neural networks, but truly understand how they think.\n",
    "\n",
    "Let‚Äôs break it down step by step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f509ce-3ab1-44d9-aa51-f16198e0415c",
   "metadata": {},
   "source": [
    "#### Step 1: Z - The weighted sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f140524-484d-4b4d-8418-109bc7ffa8b2",
   "metadata": {},
   "source": [
    "In each neuron of a hidden layer, the first operation is to compute a weighted sum of the inputs, plus a bias. This is known as:\n",
    "\n",
    "$$Z = \\sum_{i=1}^{n} w_i x_i+ b$$\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x_i$ : input values (e.g., features from the dataset) each dataset has a specific number of input values $(n)$\n",
    "- $w_i$: weights (learned parameters)\n",
    "- $b$ bias (a constant term that shifts the result)\n",
    "- $Z$: the raw output of the neuron before activation\n",
    "\n",
    "\n",
    "Weights $w_i$ are typically initialized with small random values to break symmetry and allow neurons to learn different features. A common strategies include standard normal values scaled by 0.01, in order to avoid large values\n",
    "\n",
    "Likewise, bias $b$ are usually initialized to zero, since they don‚Äôt cause symmetry issues and still allow the network to learn effectively.\n",
    "\n",
    "Think of $Z$ as the score or signal strength that the neuron is receiving. But this raw signal may be too large, too small, or even negative ‚Äî that‚Äôs why we apply an activation function afterward to shape it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba23b5-cc28-4d6b-9296-c01f62e402f2",
   "metadata": {},
   "source": [
    "#### Step 2: Applying the Activation Function $f$\n",
    "\n",
    "After computing the weighted sum $Z$, the neuron applies an **activation function** to decide what value to pass forward:\n",
    "\n",
    "$$A = f(Z)$$\n",
    "\n",
    "This function introduces **non-linearity** into the network, allowing it to learn complex patterns like curves, combinations, and interactions between inputs. Without activation functions, the network would just be a **linear function**, no matter how many layers it has. Activation functions allow the model to approximate **non-linear** functions, which is essential for solving real-world problems.\r\n",
    "\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab2fd40-eadc-4ece-b870-dcfd565c3e8b",
   "metadata": {},
   "source": [
    "##### Common Activation Functions\n",
    "\n",
    "- ReLU (Rectified Linear Unit):\n",
    "  $$f(Z) = \\max(0, Z)$$\n",
    "\n",
    "- Sigmoid\n",
    "  $$f(Z) = \\frac{1}{1 + e^{-Z}}$$\n",
    "\n",
    "- Tanh\n",
    "\n",
    "  $$f(Z) = \\frac{e^Z - e^{-Z}}{e^Z + e^{-Z}}$$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e7790-9ccf-423d-9295-7c22dbf9d3cb",
   "metadata": {},
   "source": [
    "The choice of **activation function** depends on the task and the layer. **ReLU** is generally the best default for hidden layers because it‚Äôs simple, fast, and works well in most deep learning models. **Sigmoid** is typically used in the output layer for binary classification, while **Tanh** can be a better choice than sigmoid for hidden layers when centered outputs (between -1 and 1) help with learning. For **regression tasks**, no activation or a linear function is often used in the output. \n",
    "\n",
    "The best choice depends on the problem, the layer type, and how well the network is learning during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc69062-ca13-4a4c-9339-f443014336da",
   "metadata": {},
   "source": [
    "#### Output Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd609fc-6678-445a-bbf7-e4d4158e64e6",
   "metadata": {},
   "source": [
    "In a single perceptron (a network with only one neuron), the activation function‚Äôs output is also the final output because there are no additional layers ‚Äî it directly makes the prediction.\n",
    "\n",
    "$$Z = \\sum_{i=1}^{n} w_i x_i + b$$\n",
    "\n",
    "$$\\hat{y} = f(Z)$$\n",
    "\n",
    "The data goes through the weighted sum $Z$ then through an activation function and that result is the prediction.\n",
    "\n",
    "If we add one hidden layer with multiple neurons, each neuron processes the input separately, and their combined outputs are passed to an output layer that makes the final prediction. If we go further and use multiple hidden layers with multiple neurons, each layer transforms the data step by step, allowing the network to learn more complex patterns. More layers and neurons give the model more \"brainpower\" to solve harder problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0f885-f78b-4778-980f-c8d29802ddc3",
   "metadata": {},
   "source": [
    "### Programming Task\n",
    "\n",
    "üöó Context\n",
    "\n",
    "You‚Äôve been hired by an automotive tech startup to help build a basic AI system that predicts whether a car should activate **emergency braking** (`1`) or **not** (`0`) based on real-time sensor inputs ‚Äî such as speed, distance to the obstacle, road condition, and more.\n",
    "\n",
    "You‚Äôre starting with a **single perceptron**, a simple model that takes in a variable number of input features and produces a binary prediction. You're not building a full model yet ‚Äî you're just running **forward propagation** with random weights to see what the perceptron predicts.\n",
    "\n",
    "üéØ Your Task\n",
    "\n",
    "Complete the function `forward_propagation(inputs, bias=0)` that simulates a forward pass of a single perceptron.\n",
    "\n",
    "Here‚Äôs what you need to do:\n",
    "\n",
    "1. The function receives:\n",
    "   - A list of numerical `inputs` (e.g., `[45.0, 2.5, 1.0]`)  *(These could represent: speed, distance to object, road friction, etc.)*\n",
    "   - An optional `bias` value (default is 0)\n",
    "  \n",
    "2. You must:\n",
    "   - Initialize a list of weights from a **normal distribution scaled by 0.01**\n",
    "   - Compute the **linear combination**:\n",
    "    \n",
    "     $$Z = \\sum_{i=1}^{n} w_i x_i + b$$\n",
    "     \n",
    "   - Apply the **sigmoid activation function** to get the prediction:\n",
    "     $$\\hat{y} = sigmoid(Z)$$\n",
    "3. Return:\n",
    "   - The list of weights\n",
    "   - The predicted value $\\hat{y}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d823c61-9bba-4347-b690-297d11488708",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e934e3-8ead-47f9-84cc-8eaa13296502",
   "metadata": {},
   "source": [
    "### PROGRAMING TASK\n",
    "\n",
    "üöó Context\n",
    "\n",
    "You‚Äôve been hired by an automotive tech startup to help build a basic AI system that predicts whether a car should activate **emergency braking** (`1`) or **not** (`0`) based on real-time sensor inputs ‚Äî such as speed, distance to the obstacle, road condition, and more.\n",
    "\n",
    "You‚Äôre starting with a **single perceptron**, a simple model that takes in a variable number of input features and produces a binary prediction. You're not building a full model yet ‚Äî you're just running **forward propagation** with random weights to see what the perceptron predicts.\n",
    "\n",
    "üéØ Your Task\n",
    "\n",
    "Complete the function `forward_propagation(inputs, bias=0)` that simulates a forward pass of a single perceptron.\n",
    "\n",
    "Here‚Äôs what you need to do:\n",
    "\n",
    "1. The function receives:\n",
    "   - A list of numerical `inputs` with **two or more values** (e.g., `[45.0, 2.5, 1.0]`)  \r\n",
    "     *(These could represent: speed, distance to object, road friction, etc.)*\n",
    "   - An optional `bias` value (default is 0)\n",
    "  \n",
    "2. You must:\n",
    "   - Initialize a list of weights from a **normal distribution scaled by 0.01**\n",
    "   - Compute the **linear combination**:\n",
    "    \n",
    "     $$Z = \\sum_{i=1}^{n} w_i x_i + b$$\n",
    "     \n",
    "   - Apply the **sigmoid activation function** to get the prediction:\n",
    "     $$\\hat{y} = sigmoid(Z)$$\n",
    "3. Return:\n",
    "   - The list of weights\n",
    "   - The predicted value $\\hat{y}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48438c6b-a1e0-49a2-b93b-06c786e8e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(inputs, bias=0):\n",
    "    \"\"\"\n",
    "    Simulates forward propagation for a single perceptron.\n",
    "\n",
    "    Parameters:\n",
    "    - inputs: list of numerical input values (length >= 2)\n",
    "    - bias: bias term (default is 0)\n",
    "\n",
    "    Returns:\n",
    "    - weights: list of randomly initialized weights\n",
    "    - prediction: output after applying the sigmoid function\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Convert inputs to a NumPy array\n",
    "    x = # your code goes here\n",
    "\n",
    "    # Step 2: Initialize weights with small random values (Normal distribution * 0.01)\n",
    "    weights = # your code goes here\n",
    "\n",
    "    # Step 3: Compute the weighted sum Z = w ‚Ä¢ x + b\n",
    "    z = # your code goes here\n",
    "\n",
    "    # Step 4: Define and apply the sigmoid activation function\n",
    "    def sigmoid(z):\n",
    "        # your code goes here\n",
    "        pass\n",
    "\n",
    "    prediction = # your code goes here\n",
    "\n",
    "    # Step 5: Return weights and prediction\n",
    "    return weights, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0737b-8f00-4baa-989e-388e4d962b1b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "You should convert the input list to a NumPy array so that mathematical operations like dot product and element-wise multiplication work correctly. Don‚Äôt forget to import `numpy`, it will also be useful to initialize the weights\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f38ee06-57cc-4d36-a57c-f196c7232c03",
   "metadata": {},
   "source": [
    "**‚úÖ Unit Tests for forward_propagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed86927-e1d0-4685-83b5-d8cdd776d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward_propagation():\n",
    "    np.random.seed(42)  # Ensures reproducibility\n",
    "\n",
    "    test_cases = {\n",
    "        \"2 inputs\": [1.0, 2.0],\n",
    "        \"5 inputs\": [0.5, 1.2, 0.7, 2.4, 1.0],\n",
    "        \"10 inputs\": list(range(1, 11))\n",
    "    }\n",
    "\n",
    "    for name, inputs in test_cases.items():\n",
    "        try:\n",
    "            weights, prediction = forward_propagation(inputs)\n",
    "\n",
    "            # Check that weights is a NumPy array\n",
    "            assert isinstance(weights, np.ndarray), f\"[{name}] Weights should be a NumPy array.\"\n",
    "\n",
    "            # Check that prediction is a float\n",
    "            assert isinstance(prediction, float) or isinstance(prediction, np.float64), f\"[{name}] Prediction should be a float.\"\n",
    "\n",
    "            # Check length of weights matches input\n",
    "            assert len(weights) == len(inputs), f\"[{name}] Number of weights should match number of inputs.\"\n",
    "\n",
    "            # Check prediction is in valid range\n",
    "            assert 0 <= prediction <= 1, f\"[{name}] Prediction should be between 0 and 1. Got {prediction}\"\n",
    "\n",
    "            print(f\"‚úÖ Test passed: {name}\")\n",
    "\n",
    "        except AssertionError as e:\n",
    "            print(f\"‚ùå Test failed: {e}\")\n",
    "    \n",
    "    # Additional test: check if changing the bias affects prediction\n",
    "    try:\n",
    "        inputs = [1.0, 2.0]\n",
    "\n",
    "        np.random.seed(42)\n",
    "        _, pred1 = forward_propagation(inputs, bias=0)\n",
    "\n",
    "        np.random.seed(42)  # Reset seed to get same weights\n",
    "        _, pred2 = forward_propagation(inputs, bias=5)\n",
    "\n",
    "        assert pred1 != pred2, (\n",
    "            f\"Prediction should change when bias changes.\\n\"\n",
    "            f\"Prediction with bias=0: {pred1}\\n\"\n",
    "            f\"Prediction with bias=5: {pred2}\"\n",
    "        )\n",
    "\n",
    "        print(\"‚úÖ Test passed: Bias impact on prediction\")\n",
    "\n",
    "    except AssertionError as e:\n",
    "        print(f\"‚ùå Test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f02c9c-194e-48e7-9a57-b51aef69fd29",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Comment</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "Everything above this point is the material available to the student. What follows are examples of incorrect code that would fail the Unit Test. The final section shows the correct solution.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b173a9-9933-442c-b3ea-b87b9d9a7671",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Case 1 (‚ùåTest failed): </b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "Weights is not a NumPy array\n",
    "\n",
    "</div></b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "404cd3b7-721b-4d8b-92b2-7d4cd65880ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Test failed: [2 inputs] Weights should be a NumPy array.\n",
      "‚ùå Test failed: [5 inputs] Weights should be a NumPy array.\n",
      "‚ùå Test failed: [10 inputs] Weights should be a NumPy array.\n",
      "‚úÖ Test passed: Bias impact on prediction\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forward_propagation(inputs, bias=0):\n",
    "    \"\"\"\n",
    "    Simulates forward propagation for a single perceptron.\n",
    "\n",
    "    Parameters:\n",
    "    - inputs: list of numerical input values (length >= 2)\n",
    "    - bias: bias term (default is 0)\n",
    "\n",
    "    Returns:\n",
    "    - weights: list of randomly initialized weights\n",
    "    - prediction: output after applying the sigmoid function\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Convert inputs to a NumPy array\n",
    "    x = np.array(inputs)\n",
    "\n",
    "    # Step 2: Initialize weights with small random values (Normal distribution * 0.01)\n",
    "    weights = [0.01 * np.random.randn() for _ in x] # ‚ùå Python list\n",
    "\n",
    "    # Step 3: Compute the weighted sum Z = w ‚Ä¢ x + b\n",
    "    z = np.dot(weights, x) + bias\n",
    "\n",
    "    # Step 4: Define and apply the sigmoid activation function\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    prediction = sigmoid(z)\n",
    "\n",
    "    # Step 5: Return weights and prediction\n",
    "    return weights, prediction\n",
    "\n",
    "test_forward_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a40445b-6ca0-44bd-8327-628d45d057d4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Case 2 (‚ùåTest failed):</b><a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "Prediction is not a float (returns a list instead)\n",
    "\n",
    "</div></b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48fdf278-16e6-4bcd-9e94-91cd50c12cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Test failed: [2 inputs] Prediction should be a float.\n",
      "‚ùå Test failed: [5 inputs] Prediction should be a float.\n",
      "‚ùå Test failed: [10 inputs] Prediction should be a float.\n",
      "‚úÖ Test passed: Bias impact on prediction\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forward_propagation(inputs, bias=0):\n",
    "    \"\"\"\n",
    "    Simulates forward propagation for a single perceptron.\n",
    "\n",
    "    Parameters:\n",
    "    - inputs: list of numerical input values (length >= 2)\n",
    "    - bias: bias term (default is 0)\n",
    "\n",
    "    Returns:\n",
    "    - weights: list of randomly initialized weights\n",
    "    - prediction: output after applying the sigmoid function\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Convert inputs to a NumPy array\n",
    "    x = np.array(inputs)\n",
    "\n",
    "    # Step 2: Initialize weights with small random values (Normal distribution * 0.01)\n",
    "    weights = np.random.randn(len(x)) * 0.01\n",
    "\n",
    "    # Step 3: Compute the weighted sum Z = w ‚Ä¢ x + b\n",
    "    z = np.dot(weights, x) + bias\n",
    "\n",
    "    # Step 4: Define and apply the sigmoid activation function\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    prediction = [sigmoid(z)]  # ‚ùå wrapped in a list\n",
    "\n",
    "    # Step 5: Return weights and prediction\n",
    "    return weights, prediction\n",
    "    return weights, prediction\n",
    "\n",
    "\n",
    "test_forward_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea9314-e0a3-4b0a-ae1c-0728544a3737",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Case 3 (‚ùåTest failed):</b><a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "Length of weights doesn't match inputs\n",
    "\n",
    "</div></b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb3f76d-09c7-40f4-8efa-35a94e80b0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Test failed: [2 inputs] Number of weights should match number of inputs.\n",
      "‚ùå Test failed: [5 inputs] Number of weights should match number of inputs.\n",
      "‚ùå Test failed: [10 inputs] Number of weights should match number of inputs.\n",
      "‚úÖ Test passed: Bias impact on prediction\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def forward_propagation(inputs, bias=0):\n",
    "    \"\"\"\n",
    "    Simulates forward propagation for a single perceptron.\n",
    "\n",
    "    Parameters:\n",
    "    - inputs: list of numerical input values (length >= 2)\n",
    "    - bias: bias term (default is 0)\n",
    "\n",
    "    Returns:\n",
    "    - weights: list of randomly initialized weights\n",
    "    - prediction: output after applying the sigmoid function\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Convert inputs to a NumPy array\n",
    "    x = np.array(inputs)\n",
    "\n",
    "    # Step 2: Initialize weights with small random values (Normal distribution * 0.01)\n",
    "    weights = np.random.randn(len(x) + 1) * 0.01  # ‚ùå one extra weight\n",
    "\n",
    "    # Step 3: Compute the weighted sum Z = w ‚Ä¢ x + b\n",
    "    z = z = np.dot(weights[:len(x)], x) + bias  # avoid crash by slicing\n",
    "\n",
    "    # Step 4: Define and apply the sigmoid activation function\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    prediction = sigmoid(z)\n",
    "\n",
    "    # Step 5: Return weights and prediction\n",
    "    return weights, prediction\n",
    "\n",
    "test_forward_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5174089d-a4ea-4d7f-9091-826cac3068db",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Case 4 (‚ùåTest failed):</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "Prediction is out of [0, 1] range (no sigmoid)\n",
    "\n",
    "</div></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff68c762-b9c9-4784-8d08-50b2505d2147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test passed: 2 inputs\n",
      "‚úÖ Test passed: 5 inputs\n",
      "‚ùå Test failed: [10 inputs] Prediction should be between 0 and 1. Got -0.4365558316088587\n",
      "‚úÖ Test passed: Bias impact on prediction\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def forward_propagation(inputs, bias=0):\n",
    "    \"\"\"\n",
    "    Simulates forward propagation for a single perceptron.\n",
    "\n",
    "    Parameters:\n",
    "    - inputs: list of numerical input values (length >= 2)\n",
    "    - bias: bias term (default is 0)\n",
    "\n",
    "    Returns:\n",
    "    - weights: list of randomly initialized weights\n",
    "    - prediction: output after applying the sigmoid function\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Convert inputs to a NumPy array\n",
    "    x = np.array(inputs)\n",
    "\n",
    "    # Step 2: Initialize weights\n",
    "    weights = np.random.randn(len(x)) * 0.01\n",
    "\n",
    "    # Step 4: Define and apply the sigmoid activation function\n",
    "    z = np.dot(weights, x) + bias\n",
    "\n",
    "    prediction = z # ‚ùå No sigmoid\n",
    "\n",
    "    return weights, prediction\n",
    "\n",
    "test_forward_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30565b6-0a05-43aa-90a3-c880a1441545",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Case 5 (‚ùåTest failed):</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "Weighted sum does not take into account the bias\n",
    "\n",
    "</div></b>\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6766491-ea4d-49fa-9c63-0a69f7113164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test passed: 2 inputs\n",
      "‚úÖ Test passed: 5 inputs\n",
      "‚úÖ Test passed: 10 inputs\n",
      "‚ùå Test failed: Prediction should change when bias changes.\n",
      "Prediction with bias=0: 0.5005504636542771\n",
      "Prediction with bias=5: 0.5005504636542771\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forward_propagation(inputs, bias=0):\n",
    "    \"\"\"\n",
    "    Simulates forward propagation for a single perceptron.\n",
    "\n",
    "    Parameters:\n",
    "    - inputs: list of numerical input values (length >= 2)\n",
    "    - bias: bias term (default is 0)\n",
    "\n",
    "    Returns:\n",
    "    - weights: list of randomly initialized weights\n",
    "    - prediction: output after applying the sigmoid function\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Convert inputs to a NumPy array\n",
    "    x = np.array(inputs)\n",
    "\n",
    "    # Step 2: Initialize weights with small random values (Normal distribution * 0.01)\n",
    "    weights = np.random.randn(len(x)) * 0.01\n",
    "\n",
    "    # Step 3: Compute the weighted sum Z = w ‚Ä¢ x + b\n",
    "    z = np.dot(weights, x) \n",
    "\n",
    "    # Step 4: Define and apply the sigmoid activation function\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    prediction = sigmoid(z)\n",
    "\n",
    "    # Step 5: Return weights and prediction\n",
    "    return weights, prediction\n",
    "\n",
    "\n",
    "test_forward_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b611e1-fab1-42e4-9c62-88cb4063f661",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b380c998-69cc-4c28-ad62-1b7506f7e84b",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Final (‚úÖ Test passed):</b> <a class=‚ÄútocSkip‚Äù></a>\n",
    "\n",
    "\n",
    "\n",
    "</div></b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c8ad118-3186-4979-8b77-bb41d5da3168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test passed: 2 inputs\n",
      "‚úÖ Test passed: 5 inputs\n",
      "‚úÖ Test passed: 10 inputs\n",
      "‚úÖ Test passed: Bias impact on prediction\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forward_propagation(inputs, bias=0):\n",
    "    \"\"\"\n",
    "    Simulates forward propagation for a single perceptron.\n",
    "\n",
    "    Parameters:\n",
    "    - inputs: list of numerical input values (length >= 2)\n",
    "    - bias: bias term (default is 0)\n",
    "\n",
    "    Returns:\n",
    "    - weights: list of randomly initialized weights\n",
    "    - prediction: output after applying the sigmoid function\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Convert inputs to a NumPy array\n",
    "    x = np.array(inputs)\n",
    "\n",
    "    # Step 2: Initialize weights with small random values (Normal distribution * 0.01)\n",
    "    weights = np.random.randn(len(x)) * 0.01\n",
    "\n",
    "    # Step 3: Compute the weighted sum Z = w ‚Ä¢ x + b\n",
    "    z = np.dot(weights, x) + bias\n",
    "\n",
    "    # Step 4: Define and apply the sigmoid activation function\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    prediction = sigmoid(z)\n",
    "\n",
    "    # Step 5: Return weights and prediction\n",
    "    return weights, prediction\n",
    "\n",
    "test_forward_propagation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a776da23-1a6a-45e6-9299-a7e75086ac08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case: 2 inputs\n",
      "Weights: [0.00647689 0.0152303 ]\n",
      "Prediction: 0.5092333208398085\n",
      "-----------\n",
      "Case: 5 inputs\n",
      "Weights: [-0.00234153 -0.00234137  0.01579213  0.00767435 -0.00469474]\n",
      "Prediction: 0.5051992548390593\n",
      "-----------\n",
      "Case: 10 inputs\n",
      "Weights: [ 0.0054256  -0.00463418 -0.0046573   0.00241962 -0.0191328  -0.01724918\n",
      " -0.00562288 -0.01012831  0.00314247 -0.00908024]\n",
      "Prediction: 0.40366867386652255\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "# Test without bias\n",
    "test_cases = {\n",
    "        \"2 inputs\": [1.0, 2.0],\n",
    "        \"5 inputs\": [0.5, 1.2, 0.7, 2.4, 1.0],\n",
    "        \"10 inputs\": list(range(1, 11))\n",
    "    }\n",
    "\n",
    "for name, inputs in test_cases.items():\n",
    "    weights, prediction = forward_propagation(inputs)\n",
    "    print(f\"Case: {name}\")\n",
    "    print(f\"Weights: {weights}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a064d2e-a7bc-4bee-a8e6-d7328eb25e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case: 2 inputs\n",
      "Weights: [-0.01412304  0.01465649]\n",
      "Prediction: 0.7340345994202142\n",
      "-----------\n",
      "Case: 5 inputs\n",
      "Weights: [-0.00225776  0.00067528 -0.01424748 -0.00544383  0.00110923]\n",
      "Prediction: 0.7266619880225438\n",
      "-----------\n",
      "Case: 10 inputs\n",
      "Weights: [-0.01150994  0.00375698 -0.00600639 -0.00291694 -0.00601707  0.01852278\n",
      " -0.00013497 -0.01057711  0.00822545 -0.01220844]\n",
      "Prediction: 0.713767216013312\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "# Test with bias = 1\n",
    "\n",
    "test_cases = {\n",
    "        \"2 inputs\": [1.0, 2.0],\n",
    "        \"5 inputs\": [0.5, 1.2, 0.7, 2.4, 1.0],\n",
    "        \"10 inputs\": list(range(1, 11))\n",
    "    }\n",
    "\n",
    "for name, inputs in test_cases.items():\n",
    "    weights, prediction = forward_propagation(inputs, bias = 1)\n",
    "    print(f\"Case: {name}\")\n",
    "    print(f\"Weights: {weights}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(\"-----------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
